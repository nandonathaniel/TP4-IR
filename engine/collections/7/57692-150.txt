Other key techniques in this field are negative sampling and word embedding . Word embedding , such as word2vec , can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset ; the position is represented as a point in a vector space . Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar . A compositional vector grammar can be thought of as probabilistic context free grammar ( PCFG ) implemented by an RNN . Recursive auto - encoders built atop word embeddings can assess sentence similarity and detect paraphrasing . Deep neural architectures provide the best results for constituency parsing , sentiment analysis , information retrieval , spoken language understanding , machine translation , contextual entity linking , writing style recognition , Text classification and others .